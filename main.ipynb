{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os.path as op\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "from model.net import Net\n",
    "from utils import NetworkData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "weight_decay = 5e-4\n",
    "num_epochs = 10\n",
    "# val_ratio = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34404 419840\n",
      "building adjacency ...\n",
      "normalize adjacency ...\n",
      "building node features ...\n",
      "building node features ...\n",
      "building edge features ...\n",
      "building edge features ...\n",
      "get data: \tnumber of users: 34404\n",
      "number of songs: 419840\n",
      "adjacency: (454244, 454244)\n",
      "Lap: (454244, 454244)\n",
      "dim of user feature: 6\n"
     ]
    }
   ],
   "source": [
    "data = NetworkData(cache=None, save=False)\n",
    "adj, L, x_user, x_item, x_train_edge, x_test_edge, y_train = data.get_data()\n",
    "data.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xh/jth0z5cd3gvbvnjcg95dy6nc0000gn/T/ipykernel_45880/1321777496.py:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x_user = x_user / x_user.sum(1, keepdims=True)\n",
      "/var/folders/xh/jth0z5cd3gvbvnjcg95dy6nc0000gn/T/ipykernel_45880/1321777496.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x_user = x_user / x_user.sum(1, keepdims=True)\n"
     ]
    }
   ],
   "source": [
    "# norm\n",
    "x_user = x_user / x_user.sum(1, keepdims=True)\n",
    "x_item = x_item / x_item.sum(1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# to tensor\n",
    "x_user = torch.from_numpy(np.float32(x_user))\n",
    "x_item = torch.from_numpy(np.float32(x_item))\n",
    "y_train = torch.from_numpy(np.array(y_train))\n",
    "\n",
    "num_user_nodes, user_input_dim = x_user.shape\n",
    "num_item_nodes, item_input_dim = x_item.shape\n",
    "hidden_dim = 500\n",
    "embedding_dim = 500\n",
    "edge_dim = len(x_train_edge['9176-86884'])\n",
    "num_nodes = num_item_nodes + num_user_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "indices = torch.from_numpy(np.array([L.row, L.col]).astype(\"int64\")).long()\n",
    "values = torch.from_numpy(L.data.astype(np.float32))\n",
    "adjacency = torch.sparse.FloatTensor(indices, values, L.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([454244, 454244])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = Net(user_input_dim, item_input_dim, hidden_dim, embedding_dim, edge_dim, use_edge_feature=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# train\n",
    "def train():\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        logits = model(adjacency, x_user, x_item, x_train_edge)\n",
    "        loss = criterion(logits, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "        acc = accuracy(logits, y_train)\n",
    "        acc_history.append(acc.item())\n",
    "        print(\"Epoch {:03d}: Loss {:.4f}, TrainAcc {:.4f}\".format(\n",
    "            epoch, loss.item(), acc.item()\n",
    "        ))\n",
    "    return loss_history, acc_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# test\n",
    "def accuracy(logits, y):\n",
    "    ones = torch.ones_like(logits)\n",
    "    zeros = torch.zeros_like(logits)\n",
    "    y_hat = torch.where(logits > 0.5, ones, logits)\n",
    "    y_hat = torch.where(logits <= 0.5, zeros, logits)\n",
    "    return (y_hat == y).sum() / len(y_hat)\n",
    "\n",
    "# plot\n",
    "from matplotlib import pyplot as plt\n",
    "def plot_loss_and_acc(loss_history, acc_history):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot(range(len(loss_history)), loss_history,\n",
    "             c=np.array([255, 71, 90]) / 255.)\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n",
    "    ax2.plot(range(len(acc_history)), acc_history,\n",
    "             c=np.array([79, 179, 255]) / 255.)\n",
    "    ax2.yaxis.tick_right()\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    plt.ylabel('TrainAcc')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title('Training Loss & Training Accuracy')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duran/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "addmm: Argument #3 (dense): Expected dim 0 size 454244, got 450416",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [18]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m loss, acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m plot_loss_and_acc(loss, acc)\n",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m----> 8\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43madjacency\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_user\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_item\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train_edge\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(logits, y_train)\n\u001B[1;32m     11\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/kkbox-music-recommendation/model/net.py:30\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, adjacency, user_feat, item_feat, edge_feature)\u001B[0m\n\u001B[1;32m     26\u001B[0m item_proj_feat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj_item(item_feat))\n\u001B[1;32m     28\u001B[0m feat \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((user_proj_feat, item_proj_feat), \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m embd_user \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgnn_user\u001B[49m\u001B[43m(\u001B[49m\u001B[43madjacency\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m embd_item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgnn_item(adjacency, feat)\n\u001B[1;32m     33\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmul(embd_user, embd_item)\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/kkbox-music-recommendation/model/gnn.py:39\u001B[0m, in \u001B[0;36mGNN.forward\u001B[0;34m(self, adjacency, features)\u001B[0m\n\u001B[1;32m     37\u001B[0m x \u001B[38;5;241m=\u001B[39m features\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# GCN layer\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m gcn1_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgcn1\u001B[49m\u001B[43m(\u001B[49m\u001B[43madjacency\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(gcn1_x)\n\u001B[1;32m     41\u001B[0m gcn2_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgcn2(adjacency, x)\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/kkbox-music-recommendation/model/gnn.py:65\u001B[0m, in \u001B[0;36mGCNLayer.forward\u001B[0;34m(self, adjacency, features)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, adjacency, features):\n\u001B[1;32m     64\u001B[0m     h \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmm(features, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight)\n\u001B[0;32m---> 65\u001B[0m     h \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43madjacency\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_bias:\n\u001B[1;32m     67\u001B[0m         h \u001B[38;5;241m=\u001B[39m h\u001B[38;5;241m.\u001B[39mclone() \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.9/site-packages/torch/sparse/__init__.py:91\u001B[0m, in \u001B[0;36mmm\u001B[0;34m(mat1, mat2)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mat1\u001B[38;5;241m.\u001B[39mis_sparse \u001B[38;5;129;01mand\u001B[39;00m mat2\u001B[38;5;241m.\u001B[39mis_sparse:\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_sparse_sparse_matmul(mat1, mat2)\n\u001B[0;32m---> 91\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sparse_mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmat1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmat2\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: addmm: Argument #3 (dense): Expected dim 0 size 454244, got 450416"
     ]
    }
   ],
   "source": [
    "loss, acc = train()\n",
    "plot_loss_and_acc(loss, acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "450416"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}